#!/usr/bin/env python3
"""
Extract data using STRICT schedule matching from WEEKLY_SCHEDULE_REFERENCE.md
This is the most accurate extraction method.
"""

import json
import csv
import re
from datetime import datetime
from collections import defaultdict

# Authoritative schedule from WEEKLY_SCHEDULE_REFERENCE.md
SCHEDULE = {
    'Saturday': [
        {
            'name': 'ØºÙ†ÙŠØ© Ø§Ù„Ø³Ø§Ø¦Ù„ Ø¨Ù…Ø§ ÙÙŠ Ù„Ø§Ù…ÙŠØ© Ø´ÙŠØ® Ø§Ù„Ø¥Ø³Ù„Ø§Ù… Ù…Ù† Ù…Ø³Ø§Ø¦Ù„',
            'type': 'Lecture',
            'author': 'Ø£Ø­Ù…Ø¯ Ø§Ù„Ù†Ø¬Ù…ÙŠ',
            'location': 'Ø¬Ø§Ù…Ø¹ Ø§Ù„ÙˆØ±ÙˆØ¯',
            'category': 'Aqeedah'
        },
        {
            'name': 'Ø§Ù„Ù…ÙˆØ±Ø¯ Ø§Ù„Ø¹Ø°Ø¨ Ø§Ù„Ø²Ù„Ø§Ù„',
            'type': 'Series',
            'author': 'Ø£Ø­Ù…Ø¯ Ø§Ù„Ù†Ø¬Ù…ÙŠ',
            'location': 'Ø¬Ø§Ù…Ø¹ Ø§Ù„ÙˆØ±ÙˆØ¯',
            'category': 'Aqeedah'
        },
        {
            'name': 'Ø¥Ø±Ø´Ø§Ø¯ Ø§Ù„Ø³Ø§Ø±ÙŠ Ø´Ø±Ø­ Ø§Ù„Ø³Ù†Ø© Ù„Ù„Ø¨Ø±Ø¨Ù‡Ø§Ø±ÙŠ',
            'type': 'Series',
            'author': 'Ø£Ø­Ù…Ø¯ Ø§Ù„Ù†Ø¬Ù…ÙŠ',
            'location': 'Ø¬Ø§Ù…Ø¹ Ø§Ù„ÙˆØ±ÙˆØ¯',
            'category': 'Aqeedah',
            'aliases': ['Ø´Ø±Ø­ Ø§Ù„Ø³Ù†Ø© Ù„Ù„Ø¨Ø±Ø¨Ù‡Ø§Ø±ÙŠ', 'Ø´Ø±Ø­ Ø§Ù„Ø³Ù†Ø©']
        },
        {
            'name': 'Ø§Ù„ØªØ­ÙØ© Ø§Ù„Ù†Ø¬Ù…ÙŠØ© Ø¨Ø´Ø±Ø­ Ø§Ù„Ø£Ø±Ø¨Ø¹ÙŠÙ† Ø§Ù„Ù†ÙˆÙˆÙŠØ©',
            'type': 'Series',
            'author': 'Ø£Ø­Ù…Ø¯ Ø§Ù„Ù†Ø¬Ù…ÙŠ',
            'location': 'Ø¬Ø§Ù…Ø¹ Ø§Ù„ÙˆØ±ÙˆØ¯',
            'category': 'Hadeeth'
        },
        {
            'name': 'Ù…Ø®ØªØµØ± Ø§Ù„Ø³ÙŠØ±Ø© Ø§Ù„Ù†Ø¨ÙˆÙŠØ©',
            'type': 'Series',
            'author': 'Ù…Ø­Ù…Ø¯ Ø¨Ù† Ø¹Ø¨Ø¯Ø§Ù„ÙˆÙ‡Ø§Ø¨',
            'location': 'Ø¬Ø§Ù…Ø¹ Ø§Ù„ÙˆØ±ÙˆØ¯',
            'category': 'Seerah'
        },
        {
            'name': 'ØªÙ†Ø¨ÙŠÙ‡ Ø§Ù„Ø§Ù†Ø§Ù… Ø¹Ù„Ù‰ Ù…Ø§ ÙÙŠ ÙƒØªØ§Ø¨ Ø³Ø¨Ù„ Ø§Ù„Ø³Ù„Ø§Ù… Ù…Ù† Ø§Ù„ÙÙˆØ§Ø¦Ø¯ ÙˆØ§Ù„Ø£Ø­ÙƒØ§Ù…',
            'type': 'Series',
            'author': 'Ø£Ø­Ù…Ø¯ Ø§Ù„Ù†Ø¬Ù…ÙŠ',
            'location': 'Ø¬Ø§Ù…Ø¹ Ø§Ù„ÙˆØ±ÙˆØ¯',
            'category': 'Fiqh',
            'aliases': ['ØªÙ†Ø¨ÙŠÙ‡ Ø§Ù„Ø§Ù†Ø§Ù…', 'Ø³Ø¨Ù„ Ø§Ù„Ø³Ù„Ø§Ù…']
        },
        {
            'name': 'Ø§Ù„ØªÙØ³ÙŠØ± Ø§Ù„Ù…ÙŠØ³Ø±',
            'type': 'Series',
            'author': 'Ù†Ø®Ø¨Ø© Ù…Ù† Ø£Ù‡Ù„ Ø§Ù„Ø¹Ù„Ù…',
            'location': 'Ø¬Ø§Ù…Ø¹ Ø§Ù„ÙˆØ±ÙˆØ¯',
            'category': 'Other'
        }
    ],
    'Sunday': [
        {
            'name': 'Ø§Ù„Ù…Ù„Ø®Øµ Ø´Ø±Ø­ ÙƒØªØ§Ø¨ Ø§Ù„ØªÙˆØ­ÙŠØ¯',
            'type': 'Series',
            'author': 'ØµØ§Ù„Ø­ Ø§Ù„ÙÙˆØ²Ø§Ù†',
            'location': 'Ø¬Ø§Ù…Ø¹ Ø§Ù„ÙˆØ±ÙˆØ¯',
            'category': 'Aqeedah',
            'aliases': ['Ø§Ù„Ù…Ù„Ø®Øµ ÙÙŠ Ø´Ø±Ø­ ÙƒØªØ§Ø¨ Ø§Ù„ØªÙˆØ­ÙŠØ¯', 'ÙƒØªØ§Ø¨ Ø§Ù„ØªÙˆØ­ÙŠØ¯']
        },
        {
            'name': 'ØªØ£Ø³ÙŠØ³ Ø§Ù„Ø£Ø­ÙƒØ§Ù… Ø´Ø±Ø­ Ø¹Ù…Ø¯Ø© Ø§Ù„Ø£Ø­ÙƒØ§Ù…',
            'type': 'Series',
            'author': 'Ø£Ø­Ù…Ø¯ Ø¨Ù† ÙŠØ­ÙŠÙ‰ Ø§Ù„Ù†Ø¬Ù…ÙŠ',
            'location': 'Ø¬Ø§Ù…Ø¹ Ø§Ù„ÙˆØ±ÙˆØ¯',
            'category': 'Hadeeth',
            'aliases': ['ØªØ£Ø³ÙŠØ³ Ø§Ù„Ø£Ø­ÙƒØ§Ù…', 'Ø¹Ù…Ø¯Ø© Ø§Ù„Ø£Ø­ÙƒØ§Ù…']
        },
        {
            'name': 'Ø§Ù„Ø£ÙÙ†Ø§Ù† Ø§Ù„Ù†Ø¯ÙŠØ©',
            'type': 'Series',
            'author': 'Ø²ÙŠØ¯ Ø¨Ù† Ù‡Ø§Ø¯ÙŠ Ø§Ù„Ù…Ø¯Ø®Ù„ÙŠ',
            'location': 'Online',
            'category': 'Fiqh',
            'aliases': ['Ø§Ù„Ø£ÙÙ†Ø§Ù† Ø§Ù„Ù†Ø¯ÙŠØ© Ø´Ø±Ø­ Ø§Ù„Ø³Ø¨Ù„ Ø§Ù„Ø³ÙˆÙŠØ©']
        }
    ],
    'Monday': [
        {
            'name': 'Ø§Ù„Ù…Ù„Ø®Øµ Ø§Ù„ÙÙ‚Ù‡ÙŠ',
            'type': 'Series',
            'author': 'ØµØ§Ù„Ø­ Ø§Ù„ÙÙˆØ²Ø§Ù†',
            'location': 'Ø¬Ø§Ù…Ø¹ Ø§Ù„ÙˆØ±ÙˆØ¯',
            'category': 'Fiqh'
        },
        {
            'name': 'ØªØ£Ø³ÙŠØ³ Ø§Ù„Ø£Ø­ÙƒØ§Ù… Ø´Ø±Ø­ Ø¹Ù…Ø¯Ø© Ø§Ù„Ø£Ø­ÙƒØ§Ù…',
            'type': 'Series',
            'author': 'Ø£Ø­Ù…Ø¯ Ø¨Ù† ÙŠØ­ÙŠÙ‰ Ø§Ù„Ù†Ø¬Ù…ÙŠ',
            'location': 'Ø¬Ø§Ù…Ø¹ Ø§Ù„ÙˆØ±ÙˆØ¯',
            'category': 'Hadeeth',
            'aliases': ['ØªØ£Ø³ÙŠØ³ Ø§Ù„Ø£Ø­ÙƒØ§Ù…', 'Ø¹Ù…Ø¯Ø© Ø§Ù„Ø£Ø­ÙƒØ§Ù…']
        },
        {
            'name': 'Ø§Ù„Ø£ÙÙ†Ø§Ù† Ø§Ù„Ù†Ø¯ÙŠØ©',
            'type': 'Series',
            'author': 'Ø²ÙŠØ¯ Ø¨Ù† Ù‡Ø§Ø¯ÙŠ Ø§Ù„Ù…Ø¯Ø®Ù„ÙŠ',
            'location': 'Online',
            'category': 'Fiqh',
            'aliases': ['Ø§Ù„Ø£ÙÙ†Ø§Ù† Ø§Ù„Ù†Ø¯ÙŠØ© Ø´Ø±Ø­ Ø§Ù„Ø³Ø¨Ù„ Ø§Ù„Ø³ÙˆÙŠØ©']
        }
    ],
    'Tuesday': [
        {
            'name': 'Ø§Ù„Ù…Ù„Ø®Øµ Ø´Ø±Ø­ ÙƒØªØ§Ø¨ Ø§Ù„ØªÙˆØ­ÙŠØ¯',
            'type': 'Series',
            'author': 'ØµØ§Ù„Ø­ Ø§Ù„ÙÙˆØ²Ø§Ù†',
            'location': 'Ø¬Ø§Ù…Ø¹ Ø§Ù„ÙˆØ±ÙˆØ¯',
            'category': 'Aqeedah',
            'aliases': ['Ø§Ù„Ù…Ù„Ø®Øµ ÙÙŠ Ø´Ø±Ø­ ÙƒØªØ§Ø¨ Ø§Ù„ØªÙˆØ­ÙŠØ¯', 'ÙƒØªØ§Ø¨ Ø§Ù„ØªÙˆØ­ÙŠØ¯']
        },
        {
            'name': 'Ù…Ø¹Ø§Ø±Ø¬ Ø§Ù„Ù‚Ø¨ÙˆÙ„ Ø´Ø±Ø­ Ù…Ù†Ø¸ÙˆÙ…Ø© Ø³Ù„Ù… Ø§Ù„ÙˆØµÙˆÙ„',
            'type': 'Series',
            'author': 'Ø­Ø§ÙØ¸ Ø­ÙƒÙ…ÙŠ',
            'location': 'Online',
            'category': 'Aqeedah',
            'aliases': ['Ù…Ù†Ø¸ÙˆÙ…Ø© Ø³Ù„Ù… Ø§Ù„ÙˆØµÙˆÙ„', 'Ø³Ù„Ù… Ø§Ù„ÙˆØµÙˆÙ„', 'Ù…Ø¹Ø§Ø±Ø¬ Ø§Ù„Ù‚Ø¨ÙˆÙ„']
        }
    ],
    'Wednesday': [
        {
            'name': 'Ø§Ù„Ù…Ù„Ø®Øµ Ø§Ù„ÙÙ‚Ù‡ÙŠ',
            'type': 'Series',
            'author': 'ØµØ§Ù„Ø­ Ø§Ù„ÙÙˆØ²Ø§Ù†',
            'location': 'Ø¬Ø§Ù…Ø¹ Ø§Ù„ÙˆØ±ÙˆØ¯',
            'category': 'Fiqh'
        },
        {
            'name': 'ØªØ£Ø³ÙŠØ³ Ø§Ù„Ø£Ø­ÙƒØ§Ù… Ø´Ø±Ø­ Ø¹Ù…Ø¯Ø© Ø§Ù„Ø£Ø­ÙƒØ§Ù…',
            'type': 'Series',
            'author': 'Ø£Ø­Ù…Ø¯ Ø¨Ù† ÙŠØ­ÙŠÙ‰ Ø§Ù„Ù†Ø¬Ù…ÙŠ',
            'location': 'Online',
            'category': 'Hadeeth',
            'aliases': ['ØªØ£Ø³ÙŠØ³ Ø§Ù„Ø£Ø­ÙƒØ§Ù…', 'Ø¹Ù…Ø¯Ø© Ø§Ù„Ø£Ø­ÙƒØ§Ù…']
        }
    ],
    'Thursday': [
        # No regular scheduled classes
    ],
    'Friday': [
        {
            'name': 'Ø®Ø·Ø¨Ø© Ø§Ù„Ø¬Ù…Ø¹Ø©',
            'type': 'Khutba',
            'author': 'Not Available',
            'location': 'Ø¬Ø§Ù…Ø¹ Ø§Ù„ÙˆØ±ÙˆØ¯',
            'category': 'Other'
        },
        {
            'name': 'ØµØ­ÙŠØ­ Ø§Ù„Ø¨Ø®Ø§Ø±ÙŠ',
            'type': 'Series',
            'author': 'Ù…Ø­Ù…Ø¯ Ø¨Ù† Ø¥Ø³Ù…Ø§Ø¹ÙŠÙ„ Ø§Ù„Ø¨Ø®Ø§Ø±ÙŠ',
            'location': 'Ø¬Ø§Ù…Ø¹ Ø§Ù„ÙˆØ±ÙˆØ¯',
            'category': 'Hadeeth'
        }
    ]
}


def parse_date(date_str):
    """Parse date string to datetime"""
    if not date_str or date_str == "Not Available":
        return None

    formats = ['%d.%m.%Y', '%d/%m/%Y', '%Y-%m-%d']
    for fmt in formats:
        try:
            return datetime.strptime(date_str.split()[0], fmt)
        except:
            continue
    return None


def get_day_name(date):
    """Get English day name from date"""
    if not date:
        return None
    days = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']
    return days[date.weekday()]


def is_online(text):
    """Detect if class is online"""
    online_indicators = ['Ø¹Ù† Ø¨ÙØ¹Ø¯', 'Ø¹Ù† Ø¨Ø¹Ø¯', 'Ø¨ÙØ¹Ø¯', 'Ø¹Ø¨Ø± Ù‚Ù†Ø§Ø©', 'Ø¹Ø¨Ø± Ø§Ù„ØªÙ„ÙŠØ¬Ø±Ø§Ù…']
    return any(indicator in text for indicator in online_indicators)


def normalize_text(text):
    """Normalize Arabic text for matching"""
    if not text:
        return ""
    # Remove tashkeel and extra spaces
    text = re.sub(r'[\u064B-\u065F]', '', text)  # Remove diacritics
    text = re.sub(r'\s+', ' ', text)  # Normalize spaces
    return text.strip()


def match_series(message_text, day_of_week, location):
    """Match message to series using schedule"""
    if not day_of_week or day_of_week not in SCHEDULE:
        return None

    normalized_text = normalize_text(message_text)
    day_series = SCHEDULE[day_of_week]

    # Filter by location first
    candidates = [s for s in day_series if s['location'] == location]

    if not candidates:
        # If no exact location match, try all series for that day
        candidates = day_series

    # Try to match series name
    best_match = None
    best_score = 0

    for series in candidates:
        # Check main name
        series_normalized = normalize_text(series['name'])
        if series_normalized in normalized_text:
            score = len(series_normalized)
            if score > best_score:
                best_score = score
                best_match = series

        # Check aliases
        if 'aliases' in series:
            for alias in series['aliases']:
                alias_normalized = normalize_text(alias)
                if alias_normalized in normalized_text:
                    score = len(alias_normalized)
                    if score > best_score:
                        best_score = score
                        best_match = series

    return best_match


def extract_serial(text):
    """Extract serial/lesson number"""
    # Arabic patterns
    patterns = [
        r'Ø§Ù„Ø¯Ø±Ø³\s+([^\n\s]+(?:\s+[^\n\s]+)?)',
        r'Ø¯Ø±Ø³\s+([^\n\s]+)',
        r'Ø§Ù„Ø­Ù„Ù‚Ø©\s+([^\n\s]+)',
    ]

    for pattern in patterns:
        match = re.search(pattern, text)
        if match:
            return match.group(1).strip()

    return 'Not Available'


def extract_subtopic(text):
    """Extract subtopic/chapter"""
    patterns = [
        r'ÙƒØªØ§Ø¨\s+([^\n]+?)(?:\n|$|\s{2})',
        r'Ø¨Ø§Ø¨\s+([^\n]+?)(?:\n|$|\s{2})',
    ]

    for pattern in patterns:
        match = re.search(pattern, text)
        if match:
            return match.group(1).strip()[:100]  # Limit length

    return 'Not Available'


def extract_topic_for_khutba(text):
    """Extract topic for Khutba messages"""
    # Look for patterns like: Ù…ÙˆØ¶ÙˆØ¹: xxx or Ø¹Ù†ÙˆØ§Ù†: xxx
    patterns = [
        r'(?:Ù…ÙˆØ¶ÙˆØ¹|Ø¹Ù†ÙˆØ§Ù†|Ø§Ù„Ø®Ø·Ø¨Ø©)\s*[:ï¼š]\s*([^\n]+)',
        r'[\[ã€]([^\]ã€‘]+)[\]ã€‘]',  # Text in brackets
    ]

    for pattern in patterns:
        match = re.search(pattern, text)
        if match:
            topic = match.group(1).strip()
            if len(topic) > 5 and len(topic) < 100:
                return topic

    return 'Not Available'


def extract_arabic_date(text):
    """Extract Hijri date"""
    patterns = [
        r'(\d{1,2}\s*[/\-]\s*\d{1,2}\s*[/\-]\s*\d{4})\s*Ù‡',
        r'(\d{1,2}\s+\w+\s+\d{4})\s*Ù‡',
    ]

    for pattern in patterns:
        match = re.search(pattern, text)
        if match:
            return match.group(1)

    return 'Not Available'


def main():
    print("\n" + "="*80)
    print("ðŸ“š SCHEDULE-BASED STRICT EXTRACTION")
    print("   Using WEEKLY_SCHEDULE_REFERENCE.md as authoritative source")
    print("="*80 + "\n")

    # Load messages
    with open('messages_parsed.json', 'r', encoding='utf-8') as f:
        messages = json.load(f)

    print(f"Loaded {len(messages)} messages\n")

    results = []
    stats = {
        'total': len(messages),
        'matched_by_schedule': 0,
        'unmatched': 0,
        'khutbas': 0,
        'by_day': defaultdict(int)
    }

    for i, msg in enumerate(messages):
        print(f"Processing {i+1}/{len(messages)}: {msg['filename'][:50]}...")

        # Parse date
        date = parse_date(msg['greg_date'])
        day_of_week = get_day_name(date) if date else None

        if day_of_week:
            stats['by_day'][day_of_week] += 1

        # Determine location
        text = msg['message_text']
        location = 'Online' if is_online(text) else 'Ø¬Ø§Ù…Ø¹ Ø§Ù„ÙˆØ±ÙˆØ¯'

        # Check if it's a Khutba (Friday sermon)
        is_khutba = (day_of_week == 'Friday' and
                     ('Ø®Ø·Ø¨Ø©' in text or 'Ø§Ù„Ø¬Ù…Ø¹Ø©' in text) and
                     'ØµØ­ÙŠØ­ Ø§Ù„Ø¨Ø®Ø§Ø±ÙŠ' not in text)

        if is_khutba:
            # Handle Khutba separately
            record = {
                'TelegramFileName': msg['filename'],
                'Type': 'Khutba',
                'Topic': extract_topic_for_khutba(text),
                'SeriesName': 'Not Available',
                'SubTopic': 'Not Available',
                'Serial': 'Not Available',
                'OriginalAuthor': 'Not Available',
                'Location/Online': location,
                'Sheikh': 'Ø­Ø³Ù† Ø¨Ù† Ù…Ø­Ù…Ø¯ Ù…Ù†ØµÙˆØ± Ø§Ù„Ø¯ØºØ±ÙŠØ±ÙŠ',
                'DateInArabic': extract_arabic_date(text),
                'DateInGreg': msg['greg_date'],
                'DayOfWeek': day_of_week or 'Unknown',
                'ClipLength': msg['clip_length'],
                'Category': 'Other',
                'MatchedBy': 'Khutba Detection',
                'doubtsStatus': 'none' if day_of_week == 'Friday' else 'not on Friday'
            }
            stats['khutbas'] += 1
        else:
            # Try to match using schedule
            matched_series = match_series(text, day_of_week, location)

            if matched_series:
                # Matched successfully
                record = {
                    'TelegramFileName': msg['filename'],
                    'Type': matched_series['type'],
                    'Topic': 'Not Available' if matched_series['type'] == 'Series' else extract_topic_for_khutba(text),
                    'SeriesName': matched_series['name'],
                    'SubTopic': extract_subtopic(text),
                    'Serial': extract_serial(text),
                    'OriginalAuthor': matched_series['author'],
                    'Location/Online': matched_series['location'],
                    'Sheikh': 'Ø­Ø³Ù† Ø¨Ù† Ù…Ø­Ù…Ø¯ Ù…Ù†ØµÙˆØ± Ø§Ù„Ø¯ØºØ±ÙŠØ±ÙŠ',
                    'DateInArabic': extract_arabic_date(text),
                    'DateInGreg': msg['greg_date'],
                    'DayOfWeek': day_of_week or 'Unknown',
                    'ClipLength': msg['clip_length'],
                    'Category': matched_series['category'],
                    'MatchedBy': f'Schedule ({day_of_week})',
                    'doubtsStatus': 'none'
                }
                stats['matched_by_schedule'] += 1
            else:
                # Could not match
                record = {
                    'TelegramFileName': msg['filename'],
                    'Type': 'Unknown',
                    'Topic': 'Not Available',
                    'SeriesName': 'Not Available',
                    'SubTopic': extract_subtopic(text),
                    'Serial': extract_serial(text),
                    'OriginalAuthor': 'Not Available',
                    'Location/Online': location,
                    'Sheikh': 'Ø­Ø³Ù† Ø¨Ù† Ù…Ø­Ù…Ø¯ Ù…Ù†ØµÙˆØ± Ø§Ù„Ø¯ØºØ±ÙŠØ±ÙŠ',
                    'DateInArabic': extract_arabic_date(text),
                    'DateInGreg': msg['greg_date'],
                    'DayOfWeek': day_of_week or 'Unknown',
                    'ClipLength': msg['clip_length'],
                    'Category': 'Other',
                    'MatchedBy': 'Not Matched',
                    'doubtsStatus': f'Could not match to schedule (Day: {day_of_week}, Location: {location})'
                }
                stats['unmatched'] += 1

        results.append(record)

    # Save to CSV
    output_file = 'extracted_lectures_schedule_based.csv'

    fieldnames = [
        'TelegramFileName', 'Type', 'Topic', 'SeriesName', 'SubTopic',
        'Serial', 'OriginalAuthor', 'Location/Online', 'Sheikh',
        'DateInArabic', 'DateInGreg', 'DayOfWeek', 'ClipLength',
        'Category', 'MatchedBy', 'doubtsStatus'
    ]

    with open(output_file, 'w', newline='', encoding='utf-8-sig') as csvfile:
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
        writer.writeheader()
        writer.writerows(results)

    print(f"\nâœ… Saved to: {output_file}")

    # Print statistics
    print("\n" + "="*80)
    print("ðŸ“Š EXTRACTION STATISTICS")
    print("="*80)
    print(f"\nTotal Messages: {stats['total']}")
    print(f"Matched by Schedule: {stats['matched_by_schedule']} ({stats['matched_by_schedule']/stats['total']*100:.1f}%)")
    print(f"Khutbas: {stats['khutbas']}")
    print(f"Unmatched: {stats['unmatched']} ({stats['unmatched']/stats['total']*100:.1f}%)")

    print("\nMessages by Day:")
    for day in ['Saturday', 'Sunday', 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday']:
        count = stats['by_day'].get(day, 0)
        if count > 0:
            print(f"  {day}: {count}")

    print("\n" + "="*80 + "\n")


if __name__ == "__main__":
    main()
