#!/usr/bin/env python3
"""
Manual-style extraction: Process series one-by-one like a human would
1. Take a series from WEEKLY_SCHEDULE_REFERENCE.md
2. Search for keywords (e.g., "ØªØ£Ø³ÙŠØ³")
3. Filter by location
4. Extract details with high accuracy
5. Move to next series
"""

import json
import csv
import re
from datetime import datetime
from collections import defaultdict

# Complete series list from WEEKLY_SCHEDULE_REFERENCE.md with search keywords
SERIES_DATABASE = [
    {
        'name': 'ØªØ£Ø³ÙŠØ³ Ø§Ù„Ø£Ø­ÙƒØ§Ù… Ø´Ø±Ø­ Ø¹Ù…Ø¯Ø© Ø§Ù„Ø£Ø­ÙƒØ§Ù…',
        'keywords': ['ØªØ£Ø³ÙŠØ³ Ø§Ù„Ø£Ø­ÙƒØ§Ù…', 'ØªØ£Ø³ÙŠØ³', 'Ø¹Ù…Ø¯Ø© Ø§Ù„Ø£Ø­ÙƒØ§Ù…', 'Ø¹Ù…Ø¯Ø©'],
        'location_masjid': True,
        'location_online': True,  # Exists in both locations
        'author': 'Ø£Ø­Ù…Ø¯ Ø¨Ù† ÙŠØ­ÙŠÙ‰ Ø§Ù„Ù†Ø¬Ù…ÙŠ',
        'category': 'Hadeeth',
        'days_masjid': ['Sunday', 'Monday'],
        'days_online': ['Wednesday']
    },
    {
        'name': 'Ø§Ù„Ù…Ù„Ø®Øµ Ø´Ø±Ø­ ÙƒØªØ§Ø¨ Ø§Ù„ØªÙˆØ­ÙŠØ¯',
        'keywords': ['ÙƒØªØ§Ø¨ Ø§Ù„ØªÙˆØ­ÙŠØ¯', 'Ø§Ù„ØªÙˆØ­ÙŠØ¯', 'Ø§Ù„Ù…Ù„Ø®Øµ Ø´Ø±Ø­ ÙƒØªØ§Ø¨'],
        'location_masjid': True,
        'location_online': False,
        'author': 'ØµØ§Ù„Ø­ Ø§Ù„ÙÙˆØ²Ø§Ù†',
        'category': 'Aqeedah',
        'days_masjid': ['Sunday', 'Tuesday']
    },
    {
        'name': 'Ø§Ù„Ù…Ù„Ø®Øµ Ø§Ù„ÙÙ‚Ù‡ÙŠ',
        'keywords': ['Ø§Ù„Ù…Ù„Ø®Øµ Ø§Ù„ÙÙ‚Ù‡ÙŠ', 'Ø§Ù„ÙÙ‚Ù‡ÙŠ'],
        'location_masjid': True,
        'location_online': False,
        'author': 'ØµØ§Ù„Ø­ Ø§Ù„ÙÙˆØ²Ø§Ù†',
        'category': 'Fiqh',
        'days_masjid': ['Monday', 'Wednesday']
    },
    {
        'name': 'Ø§Ù„Ø£ÙÙ†Ø§Ù† Ø§Ù„Ù†Ø¯ÙŠØ©',
        'keywords': ['Ø§Ù„Ø£ÙÙ†Ø§Ù† Ø§Ù„Ù†Ø¯ÙŠØ©', 'Ø§Ù„Ø£ÙÙ†Ø§Ù†', 'Ø§Ù„Ø³Ø¨Ù„ Ø§Ù„Ø³ÙˆÙŠØ©'],
        'location_masjid': False,
        'location_online': True,
        'author': 'Ø²ÙŠØ¯ Ø¨Ù† Ù‡Ø§Ø¯ÙŠ Ø§Ù„Ù…Ø¯Ø®Ù„ÙŠ',
        'category': 'Fiqh',
        'days_online': ['Sunday', 'Monday']
    },
    {
        'name': 'Ù…Ø¹Ø§Ø±Ø¬ Ø§Ù„Ù‚Ø¨ÙˆÙ„ Ø´Ø±Ø­ Ù…Ù†Ø¸ÙˆÙ…Ø© Ø³Ù„Ù… Ø§Ù„ÙˆØµÙˆÙ„',
        'keywords': ['Ø³Ù„Ù… Ø§Ù„ÙˆØµÙˆÙ„', 'Ù…Ù†Ø¸ÙˆÙ…Ø©', 'Ù…Ø¹Ø§Ø±Ø¬ Ø§Ù„Ù‚Ø¨ÙˆÙ„'],
        'location_masjid': False,
        'location_online': True,
        'author': 'Ø­Ø§ÙØ¸ Ø­ÙƒÙ…ÙŠ',
        'category': 'Aqeedah',
        'days_online': ['Tuesday']
    },
    {
        'name': 'Ø§Ù„ØªÙØ³ÙŠØ± Ø§Ù„Ù…ÙŠØ³Ø±',
        'keywords': ['Ø§Ù„ØªÙØ³ÙŠØ± Ø§Ù„Ù…ÙŠØ³Ø±', 'Ø§Ù„ØªÙØ³ÙŠØ±', 'Ø³ÙˆØ±Ø©'],
        'location_masjid': True,
        'location_online': False,
        'author': 'Ù†Ø®Ø¨Ø© Ù…Ù† Ø£Ù‡Ù„ Ø§Ù„Ø¹Ù„Ù…',
        'category': 'Other',
        'days_masjid': ['Saturday']
    },
    {
        'name': 'Ø¥Ø±Ø´Ø§Ø¯ Ø§Ù„Ø³Ø§Ø±ÙŠ Ø´Ø±Ø­ Ø§Ù„Ø³Ù†Ø© Ù„Ù„Ø¨Ø±Ø¨Ù‡Ø§Ø±ÙŠ',
        'keywords': ['Ø´Ø±Ø­ Ø§Ù„Ø³Ù†Ø©', 'Ø§Ù„Ø¨Ø±Ø¨Ù‡Ø§Ø±ÙŠ', 'Ø¥Ø±Ø´Ø§Ø¯ Ø§Ù„Ø³Ø§Ø±ÙŠ'],
        'location_masjid': True,
        'location_online': False,
        'author': 'Ø£Ø­Ù…Ø¯ Ø§Ù„Ù†Ø¬Ù…ÙŠ',
        'category': 'Aqeedah',
        'days_masjid': ['Saturday']
    },
    {
        'name': 'ØµØ­ÙŠØ­ Ø§Ù„Ø¨Ø®Ø§Ø±ÙŠ',
        'keywords': ['ØµØ­ÙŠØ­ Ø§Ù„Ø¨Ø®Ø§Ø±ÙŠ', 'Ø§Ù„Ø¨Ø®Ø§Ø±ÙŠ'],
        'location_masjid': True,
        'location_online': False,
        'author': 'Ù…Ø­Ù…Ø¯ Ø¨Ù† Ø¥Ø³Ù…Ø§Ø¹ÙŠÙ„ Ø§Ù„Ø¨Ø®Ø§Ø±ÙŠ',
        'category': 'Hadeeth',
        'days_masjid': ['Friday']
    },
    {
        'name': 'Ø§Ù„Ù…ÙˆØ±Ø¯ Ø§Ù„Ø¹Ø°Ø¨ Ø§Ù„Ø²Ù„Ø§Ù„',
        'keywords': ['Ø§Ù„Ù…ÙˆØ±Ø¯ Ø§Ù„Ø¹Ø°Ø¨', 'Ø§Ù„Ø²Ù„Ø§Ù„'],
        'location_masjid': True,
        'location_online': False,
        'author': 'Ø£Ø­Ù…Ø¯ Ø§Ù„Ù†Ø¬Ù…ÙŠ',
        'category': 'Aqeedah',
        'days_masjid': ['Saturday']
    },
    {
        'name': 'Ø§Ù„ØªØ­ÙØ© Ø§Ù„Ù†Ø¬Ù…ÙŠØ© Ø¨Ø´Ø±Ø­ Ø§Ù„Ø£Ø±Ø¨Ø¹ÙŠÙ† Ø§Ù„Ù†ÙˆÙˆÙŠØ©',
        'keywords': ['Ø§Ù„ØªØ­ÙØ© Ø§Ù„Ù†Ø¬Ù…ÙŠØ©', 'Ø§Ù„Ø£Ø±Ø¨Ø¹ÙŠÙ† Ø§Ù„Ù†ÙˆÙˆÙŠØ©', 'Ø§Ù„Ù†ÙˆÙˆÙŠØ©'],
        'location_masjid': True,
        'location_online': False,
        'author': 'Ø£Ø­Ù…Ø¯ Ø§Ù„Ù†Ø¬Ù…ÙŠ',
        'category': 'Hadeeth',
        'days_masjid': ['Saturday']
    },
    {
        'name': 'Ù…Ø®ØªØµØ± Ø§Ù„Ø³ÙŠØ±Ø© Ø§Ù„Ù†Ø¨ÙˆÙŠØ©',
        'keywords': ['Ù…Ø®ØªØµØ± Ø§Ù„Ø³ÙŠØ±Ø©', 'Ø§Ù„Ø³ÙŠØ±Ø© Ø§Ù„Ù†Ø¨ÙˆÙŠØ©'],
        'location_masjid': True,
        'location_online': False,
        'author': 'Ù…Ø­Ù…Ø¯ Ø¨Ù† Ø¹Ø¨Ø¯Ø§Ù„ÙˆÙ‡Ø§Ø¨',
        'category': 'Seerah',
        'days_masjid': ['Saturday']
    },
    {
        'name': 'ØªÙ†Ø¨ÙŠÙ‡ Ø§Ù„Ø§Ù†Ø§Ù… Ø¹Ù„Ù‰ Ù…Ø§ ÙÙŠ ÙƒØªØ§Ø¨ Ø³Ø¨Ù„ Ø§Ù„Ø³Ù„Ø§Ù… Ù…Ù† Ø§Ù„ÙÙˆØ§Ø¦Ø¯ ÙˆØ§Ù„Ø£Ø­ÙƒØ§Ù…',
        'keywords': ['ØªÙ†Ø¨ÙŠÙ‡ Ø§Ù„Ø§Ù†Ø§Ù…', 'Ø³Ø¨Ù„ Ø§Ù„Ø³Ù„Ø§Ù…'],
        'location_masjid': True,
        'location_online': False,
        'author': 'Ø£Ø­Ù…Ø¯ Ø§Ù„Ù†Ø¬Ù…ÙŠ',
        'category': 'Fiqh',
        'days_masjid': ['Saturday']
    },
    {
        'name': 'ØºÙ†ÙŠØ© Ø§Ù„Ø³Ø§Ø¦Ù„ Ø¨Ù…Ø§ ÙÙŠ Ù„Ø§Ù…ÙŠØ© Ø´ÙŠØ® Ø§Ù„Ø¥Ø³Ù„Ø§Ù… Ù…Ù† Ù…Ø³Ø§Ø¦Ù„',
        'keywords': ['ØºÙ†ÙŠØ© Ø§Ù„Ø³Ø§Ø¦Ù„', 'Ù„Ø§Ù…ÙŠØ© Ø´ÙŠØ® Ø§Ù„Ø¥Ø³Ù„Ø§Ù…'],
        'location_masjid': True,
        'location_online': False,
        'author': 'Ø£Ø­Ù…Ø¯ Ø§Ù„Ù†Ø¬Ù…ÙŠ',
        'category': 'Aqeedah',
        'days_masjid': ['Saturday']
    }
]


def parse_date(date_str):
    """Parse date string"""
    if not date_str or date_str == "Not Available":
        return None
    formats = ['%d.%m.%Y', '%d/%m/%Y', '%Y-%m-%d']
    for fmt in formats:
        try:
            return datetime.strptime(date_str.split()[0], fmt)
        except:
            continue
    return None


def get_day_name(date):
    """Get English day name"""
    if not date:
        return None
    days = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']
    return days[date.weekday()]


def is_online(text):
    """Detect if online"""
    return any(x in text for x in ['Ø¹Ù† Ø¨ÙØ¹Ø¯', 'Ø¹Ù† Ø¨Ø¹Ø¯', 'Ø¨ÙØ¹Ø¯', 'Ø¹Ø¨Ø± Ù‚Ù†Ø§Ø©', 'Ø¹Ø¨Ø± Ø§Ù„ØªÙ„ÙŠØ¬Ø±Ø§Ù…'])


def extract_serial(text):
    """Extract serial/lesson number"""
    patterns = [
        r'Ø§Ù„Ø¯Ø±Ø³\s+([^\n\s]+(?:\s+[^\n\s]+)?)',
        r'Ø¯Ø±Ø³\s+([^\n\s]+)',
        r'Ø§Ù„Ø­Ù„Ù‚Ø©\s+([^\n\s]+)',
    ]
    for pattern in patterns:
        match = re.search(pattern, text)
        if match:
            return match.group(1).strip()
    return 'Not Available'


def extract_subtopic(text):
    """Extract subtopic/chapter"""
    patterns = [
        r'ÙƒØªØ§Ø¨\s+([^\n]+?)(?:\n|$|\s{2})',
        r'Ø¨Ø§Ø¨\s+([^\n]+?)(?:\n|$|\s{2})',
    ]
    for pattern in patterns:
        match = re.search(pattern, text)
        if match:
            return match.group(1).strip()[:100]
    return 'Not Available'


def extract_arabic_date(text):
    """Extract Hijri date"""
    patterns = [
        r'(\d{1,2}\s*[/\-]\s*\d{1,2}\s*[/\-]\s*\d{4})\s*Ù‡',
        r'(\d{1,2}\s+\w+\s+\d{4})\s*Ù‡',
    ]
    for pattern in patterns:
        match = re.search(pattern, text)
        if match:
            return match.group(1)
    return 'Not Available'


def main():
    print("\n" + "="*80)
    print("ğŸ¯ MANUAL-STYLE SERIES-BY-SERIES EXTRACTION")
    print("   Processing like a human: one series at a time")
    print("="*80 + "\n")

    # Load messages
    with open('5feb26_messages_parsed.json', 'r', encoding='utf-8') as f:
        messages = json.load(f)

    print(f"Loaded {len(messages)} messages\n")

    # Track which messages have been matched
    matched_messages = set()
    all_results = []

    # Process each series one by one
    for series_idx, series in enumerate(SERIES_DATABASE, 1):
        print(f"\n{'='*80}")
        print(f"[{series_idx}/{len(SERIES_DATABASE)}] Processing: {series['name']}")
        print(f"{'='*80}")

        # Determine which locations to check
        locations_to_check = []
        if series.get('location_masjid'):
            locations_to_check.append('Ø¬Ø§Ù…Ø¹ Ø§Ù„ÙˆØ±ÙˆØ¯')
        if series.get('location_online'):
            locations_to_check.append('Online')

        for location in locations_to_check:
            print(f"\nğŸ“ Location: {location}")
            print(f"ğŸ” Searching for keywords: {', '.join(series['keywords'][:3])}...")

            series_matches = []

            # Search through all messages
            for msg_idx, msg in enumerate(messages):
                # Skip if already matched
                if msg_idx in matched_messages:
                    continue

                text = msg['message_text']
                filename = msg['filename']
                combined_text = f"{text} {filename}".lower()

                # Check location match
                msg_location = 'Online' if is_online(text) else 'Ø¬Ø§Ù…Ø¹ Ø§Ù„ÙˆØ±ÙˆØ¯'
                if msg_location != location:
                    continue

                # Check if any keyword matches
                keyword_match = False
                for keyword in series['keywords']:
                    if keyword.lower() in combined_text:
                        keyword_match = True
                        break

                if not keyword_match:
                    continue

                # Parse date and day
                date = parse_date(msg['greg_date'])
                day_of_week = get_day_name(date)

                # Optional: validate day of week if we have date
                expected_days = series.get(f"days_{'online' if location == 'Online' else 'masjid'}", [])
                if day_of_week and expected_days and day_of_week not in expected_days:
                    # Day doesn't match schedule, but include with doubt
                    doubt = f"Day mismatch: {day_of_week} (expected: {', '.join(expected_days)})"
                else:
                    doubt = "none"

                # Extract details
                record = {
                    'TelegramFileName': filename,
                    'Type': 'Series',
                    'Topic': 'Not Available',
                    'SeriesName': series['name'],
                    'SubTopic': extract_subtopic(text),
                    'Serial': extract_serial(text),
                    'OriginalAuthor': series['author'],
                    'Location/Online': location,
                    'Sheikh': 'Ø­Ø³Ù† Ø¨Ù† Ù…Ø­Ù…Ø¯ Ù…Ù†ØµÙˆØ± Ø§Ù„Ø¯ØºØ±ÙŠØ±ÙŠ',
                    'DateInArabic': extract_arabic_date(text),
                    'DateInGreg': msg['greg_date'],
                    'DayOfWeek': day_of_week or 'Unknown',
                    'ClipLength': msg['clip_length'],
                    'Category': series['category'],
                    'MatchedBy': f'Manual-style ({series_idx})',
                    'doubtsStatus': doubt
                }

                series_matches.append((msg_idx, record))
                print(f"   âœ“ {filename[:50]:50s} | {day_of_week or 'N/A':9s} | {record['Serial'][:20]}")

            # Add all matches for this series/location
            for msg_idx, record in series_matches:
                matched_messages.add(msg_idx)
                all_results.append(record)

            print(f"\n   Found {len(series_matches)} lessons for {series['name']} at {location}")

    # Handle Khutbas separately
    print(f"\n{'='*80}")
    print(f"[Special] Processing Khutbas (Friday Sermons)")
    print(f"{'='*80}\n")

    khutba_count = 0
    for msg_idx, msg in enumerate(messages):
        if msg_idx in matched_messages:
            continue

        text = msg['message_text']
        date = parse_date(msg['greg_date'])
        day_of_week = get_day_name(date)

        # Check if it's a Khutba
        if ('Ø®Ø·Ø¨Ø©' in text or 'Ø§Ù„Ø¬Ù…Ø¹Ø©' in text) and 'ØµØ­ÙŠØ­ Ø§Ù„Ø¨Ø®Ø§Ø±ÙŠ' not in text:
            location = 'Online' if is_online(text) else 'Ø¬Ø§Ù…Ø¹ Ø§Ù„ÙˆØ±ÙˆØ¯'

            # Extract topic from Khutba
            topic = 'Not Available'
            topic_patterns = [
                r'[\[ã€]([^\]ã€‘]+)[\]ã€‘]',
                r'Ø¹Ù†ÙˆØ§Ù†[:\s]+([^\n]+)',
            ]
            for pattern in topic_patterns:
                match = re.search(pattern, text)
                if match:
                    topic = match.group(1).strip()
                    break

            record = {
                'TelegramFileName': msg['filename'],
                'Type': 'Khutba',
                'Topic': topic,
                'SeriesName': 'Not Available',
                'SubTopic': 'Not Available',
                'Serial': 'Not Available',
                'OriginalAuthor': 'Not Available',
                'Location/Online': location,
                'Sheikh': 'Ø­Ø³Ù† Ø¨Ù† Ù…Ø­Ù…Ø¯ Ù…Ù†ØµÙˆØ± Ø§Ù„Ø¯ØºØ±ÙŠØ±ÙŠ',
                'DateInArabic': extract_arabic_date(text),
                'DateInGreg': msg['greg_date'],
                'DayOfWeek': day_of_week or 'Unknown',
                'ClipLength': msg['clip_length'],
                'Category': 'Other',
                'MatchedBy': 'Khutba Detection',
                'doubtsStatus': 'none' if day_of_week == 'Friday' else 'not on Friday'
            }

            matched_messages.add(msg_idx)
            all_results.append(record)
            khutba_count += 1
            print(f"   âœ“ {msg['filename'][:50]:50s} | {topic[:30]}")

    print(f"\n   Found {khutba_count} Khutbas")

    # Add unmatched messages
    print(f"\n{'='*80}")
    print(f"[Remaining] Unmatched Messages")
    print(f"{'='*80}\n")

    unmatched_count = 0
    for msg_idx, msg in enumerate(messages):
        if msg_idx in matched_messages:
            continue

        text = msg['message_text']
        date = parse_date(msg['greg_date'])
        day_of_week = get_day_name(date)
        location = 'Online' if is_online(text) else 'Ø¬Ø§Ù…Ø¹ Ø§Ù„ÙˆØ±ÙˆØ¯'

        record = {
            'TelegramFileName': msg['filename'],
            'Type': 'Unknown',
            'Topic': 'Not Available',
            'SeriesName': 'Not Available',
            'SubTopic': extract_subtopic(text),
            'Serial': extract_serial(text),
            'OriginalAuthor': 'Not Available',
            'Location/Online': location,
            'Sheikh': 'Ø­Ø³Ù† Ø¨Ù† Ù…Ø­Ù…Ø¯ Ù…Ù†ØµÙˆØ± Ø§Ù„Ø¯ØºØ±ÙŠØ±ÙŠ',
            'DateInArabic': extract_arabic_date(text),
            'DateInGreg': msg['greg_date'],
            'DayOfWeek': day_of_week or 'Unknown',
            'ClipLength': msg['clip_length'],
            'Category': 'Other',
            'MatchedBy': 'Unmatched',
            'doubtsStatus': 'Could not match to any series'
        }

        all_results.append(record)
        unmatched_count += 1

    print(f"   {unmatched_count} messages could not be matched to any series")

    # Save to CSV
    output_file = '5feb26_extracted_lectures_manual_style.csv'

    fieldnames = [
        'TelegramFileName', 'Type', 'Topic', 'SeriesName', 'SubTopic',
        'Serial', 'OriginalAuthor', 'Location/Online', 'Sheikh',
        'DateInArabic', 'DateInGreg', 'DayOfWeek', 'ClipLength',
        'Category', 'MatchedBy', 'doubtsStatus'
    ]

    with open(output_file, 'w', newline='', encoding='utf-8-sig') as csvfile:
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
        writer.writeheader()
        writer.writerows(all_results)

    # Print summary
    print(f"\n{'='*80}")
    print("ğŸ“Š EXTRACTION SUMMARY")
    print(f"{'='*80}")

    total = len(all_results)
    series_count = sum(1 for r in all_results if r['Type'] == 'Series')
    khutba_count_final = sum(1 for r in all_results if r['Type'] == 'Khutba')
    unknown = sum(1 for r in all_results if r['Type'] == 'Unknown')

    print(f"\nTotal Messages: {total}")
    print(f"âœ… Matched to Series: {series_count} ({series_count/total*100:.1f}%)")
    print(f"âœ… Khutbas: {khutba_count_final} ({khutba_count_final/total*100:.1f}%)")
    print(f"â“ Unmatched: {unknown} ({unknown/total*100:.1f}%)")
    print(f"\nğŸ’¾ Saved to: {output_file}")

    # Series breakdown
    series_counts = defaultdict(int)
    for r in all_results:
        if r['Type'] == 'Series':
            key = f"{r['SeriesName']}|{r['Location/Online']}"
            series_counts[key] += 1

    print(f"\nğŸ“š Series Breakdown ({len(series_counts)} unique series):")
    for series_key, count in sorted(series_counts.items(), key=lambda x: -x[1]):
        parts = series_key.split('|')
        print(f"   {parts[0][:55]:55s} | {parts[1]:15s} | {count:3d} lessons")

    print(f"\n{'='*80}\n")


if __name__ == "__main__":
    main()
